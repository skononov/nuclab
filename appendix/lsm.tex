\label{app:lsq}
Метод наименьших квадратов является одним из методов статистического анализа для
оценки неизвестных величин по результатам измерений, содержащих случайные ошибки.
Он является частным случаем метода максимума правдоподобия в случае нормального
распределения измеренных величин.

Рассмотрим простой частный случай --- нахождение линейной зависимости $y=f(x)=kx+b$ 
для набора из пар измерений $(x_i, y_i)$, $i=1,2,...,n$, при 
одинаковой ошибке $y_i$ и малой ошибке $x_i$. Составим сумму квадратов 
отклонений $(y_i-f(x_i))$:
\begin{equation}
S=\sum_{i=1}^n (y_i-kx_i-b)^2.
\label{app:lsq:sum}
\end{equation}

Найдем такие значения $k$ и $b$, при которых сумма $S$ будет минимальна. Условия 
минимума $S$ будут
\begin{eqnarray*}
\frac{\partial S}{\partial k} =& -2\sum_{i=1}^n (y_i-kx_i-b)x_i &= 0,\\
\frac{\partial S}{\partial b} =& -2\sum_{i=1}^n (y_i-kx_i-b) &= 0.
\end{eqnarray*}
Эти уравнения можно переписать в виде
\begin{eqnarray*}
\sum_{i=1}^n y_i x_i &=& k\sum_{i=1}^n x_i^2 + b\sum_{i=1}^n x_i,\\
\sum_{i=1}^n y_i     &=& k\sum_{i=1}^n x_i + nb.\\
\end{eqnarray*}

Для краткости обозначим $\bar{z} = \frac{1}{n}\sum_{i=1}^n z_i$, где $z$ --- это 
$x$, $x^2$ или $y x$. Выражения для искомых параметров будут
\begin{eqnarray*}
k &=& \frac{\overline{xy}-\bar{x}\bar{y}}{\overline{x^2}-\bar{x}^2},\\
b &=& \bar{y}-k\bar{x}.\\
\end{eqnarray*}

В случае, если величины $y_i$ имеют разные ошибки $\sigma_i$, то все усреднения
проводятся с весами $1/\sigma_i^2$, т.е.
\[\bar{z} = \frac{\displaystyle \sum_{i=1}^n \frac{z_i}{\sigma_i^2}}{\displaystyle \sum_{i=1}^n \frac{1}{\sigma_i^2}}.\]

Описанную процедуру называют подгонкой методом наименьших квадратов.

В случае гауссовского распределения $y_i$ величина
\begin{equation}
\chi^2 = \sum_{i=1}^n \frac{(y_i-k x_i - b)^2}{\sigma_i^2}
\label{app:lsq:chi2}
\end{equation}
подчиняется распределению хи-квадрат с числом степеней свободы
$\textrm{ndf}=n-2$.
Уменьшение степеней свободы на два по сравнению с количеством измерений
связано с тем, что в выражении (\ref{app:lsq:chi2}) $k$ и $b$ определяются
из самих эксперементальных данных. Вследствие
этого в сумме (\ref{app:lsq:chi2}) число независимых случайных 
величин уменьшается на число определяемых параметров, то есть на два.

Если ошибки $\sigma_i$ одинаковы для всех $i$ и равны $\sigma$, то 
\[\chi^2 = S/\sigma^2.\]
Ошибка $\sigma$ далеко не всегда известна {\em a priori}, но её можно
определить из того же экперимента.
Приемлемой точности в определении $\sigma$ можно добиться, имея
либо достаточно большую выборку из $n$ измерений, либо
повторив эту серию измерений многократно и получив 
измерение $\overline{S}$ с требуемой точностью. Тогда мы можем
записать
\[\overline{S}/\sigma^2 = \overline{\chi^2} \equiv n-2.\]

В итоге имеем
\begin{equation}
\sigma=\sqrt{\frac{\overline{S}}{n-2}}.
\label{app:lsq:sigma}
\end{equation}
